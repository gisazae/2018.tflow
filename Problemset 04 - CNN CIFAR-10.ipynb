{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from rlx.ml import Batches, show_image_mosaic\n",
    "from rlx import utils\n",
    "from time import time\n",
    "from rlx.utils import humanbytes\n",
    "import tflearn, psutil, gc\n",
    "%matplotlib inline\n",
    "gc.collect()\n",
    "print \"free mem\", humanbytes(psutil.virtual_memory().free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Exercise 1: Get CIFAR-10\n",
    "\n",
    "download the dataset from https://www.cs.toronto.edu/~kriz/cifar.html and create \n",
    "\n",
    "- a function to load selected batches. the argument `batches` is a list. images must `numpy arrays` with **pixel values between 0 and 1**\n",
    "- a function to make train/test splits. `train_pct` sets the percentage of images for train (for instace `0.8` is 80%). `shuffle=True` means that train and test are random partitions (if `false` then the train partition will be the first part of the data).\n",
    "\n",
    "with `train_pct=.8`, variables names and shapes must be as follows:\n",
    "\n",
    "\n",
    "        VARIABLE NAME   SHAPE\n",
    "        \n",
    "        imgs            (10000, 32, 32, 3)\n",
    "        labels          (10000,)\n",
    "        onehot          (10000, 10)\n",
    "\n",
    "        train_imgs      (8000, 32, 32, 3)\n",
    "        train_labels    (8000,)\n",
    "        train_ohlabs    (8000, 10)\n",
    "\n",
    "        test_imgs       (2000, 32, 32, 3)\n",
    "        test_labels     (2000,)\n",
    "        test_ohlabs     (2000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(batches = [1,2,3,4,5]):\n",
    "    \n",
    "    cifar10_dir =\"/mnt/cifar-10-batches-py/\"   # this is where you downloaded CIFAR10\n",
    "\n",
    "    def onehot_labels(labels):\n",
    "        return np.eye(10)[labels]\n",
    "\n",
    "    imgs   = ...\n",
    "    labels = ...\n",
    "    ohlabs = ...\n",
    "    \n",
    "    return imgs, labels, ohlabs\n",
    "\n",
    "def train_test_split(imgs, labels, ohlabs, train_pct=.8, shuffle=True):\n",
    "\n",
    "    train_imgs   = ...\n",
    "    train_ohlabs = ...\n",
    "    train_labels = ...\n",
    "\n",
    "    test_imgs   = ...\n",
    "    test_ohlabs = ...\n",
    "    test_labels = ...\n",
    "\n",
    "    return train_imgs, train_labels, train_ohlabs, test_imgs, test_labels, test_ohlabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data with your functions. we create the TEST/TRAIN split. ** we will use only batch 1  ** and select randomly **80%** for **TRAIN** and the rest for **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels, ohlabs = load_cifar(batches = [1])\n",
    "d = train_test_split(imgs, labels, ohlabs)\n",
    "train_imgs, train_labels, train_ohlabs, test_imgs, test_labels, test_ohlabs = d\n",
    "\n",
    "cnames = [\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"boat\", \"truck\"]\n",
    "\n",
    "print \"imgs  \", imgs.shape, \"min\", np.min(imgs), \"max\", np.max(imgs)\n",
    "print \"labels\", labels.shape\n",
    "print \"onehot\", ohlabs.shape\n",
    "print \"train_imgs  \", train_imgs.shape\n",
    "print \"train_labels\", train_labels.shape\n",
    "print \"train_ohlabs\", train_ohlabs.shape\n",
    "print \"test_imgs   \", test_imgs.shape\n",
    "print \"test_labels \", test_labels.shape\n",
    "print \"test_ohlabs \", test_ohlabs.shape\n",
    "gc.collect()\n",
    "print \"free mem\", humanbytes(psutil.virtual_memory().free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_mosaic(train_imgs, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Create TF vars for following network\n",
    "\n",
    "| layer   | input_size  | output_size | filter_size  | stride | n_filters |activation| var sizes  | params |\n",
    "| ------- |:-----------:|:-----------:|:------------:|:------:|:---------:|:--------:|:--------------:| |\n",
    "| conv1   | 32x32x3     | 32x32x9     | 5x5          |1       | 15        | relu     | W1 = [5,5,3,15]<br/> b = [15]||\n",
    "| conv2   | 32x32x15    | 16x16x18    | 5x5          |2       | 18        | relu     | W2 = [5,5,15,18]<br/> b = [18]||\n",
    "| conv3   | 16x16x18    | 8x8x20      | 3x3          |2       | 20        | relu     | W2 = [3,3,18,20]<br/> b =[20]||\n",
    "| maxpool | 8x8x20      | 4x4x20      |              |        |           |          | | k = 2 |\n",
    "| fc      | 4x4x20      |    100      |              |        |           | relu     | W3 = [320,100] <br/>b=[100]||\n",
    "| dropout | 100         |   100       |              |        |           |          | | pkeep = .75 |\n",
    "| output  | 100         |   10        |              |        |           | softmax  | W4 = [100,10] <br/>b=[10]||\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_size, f2_size, f3_size, fc_size    = 5, 5, 3, 100\n",
    "c1_nfilters, c2_nfilters, c3_nfilters = 15, 18, 20\n",
    "c1_stride, c2_stride, c3_stride       = 1, 2, 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TF_vars():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.name_scope(\"data\"):\n",
    "        X  = ...\n",
    "        Y  = ...\n",
    "\n",
    "    with tf.name_scope(\"weights_biases\"):\n",
    "\n",
    "        W1 = ...  \n",
    "        b1 = ...\n",
    "\n",
    "        W2 = ...\n",
    "        b2 = ...\n",
    "\n",
    "        W3 = ...\n",
    "        b3 = ...\n",
    "\n",
    "        W4 = ...\n",
    "        b4 = ...\n",
    "\n",
    "        W5 = ...\n",
    "        b5 = ...\n",
    "    \n",
    "    return X, Y, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vars = get_TF_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: create TF graph\n",
    "\n",
    "- use dropout `pkeep` as given in the argument.\n",
    "- use `tf.nn.relu`, `tf.matmul`, `tf.nn.dropout`, `tf.nn.softmax` for layers\n",
    "- use `tf.nn.softmax_cross_entropy_with_logits` to compute the loss\n",
    "- use `tf.train.AdamOptimizer` as optimizer with `learning_rate` from the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TF_graph(tf_vars, pkeep=0.75, learning_rate=0.001):\n",
    "    \n",
    "    X, Y, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = tf_vars\n",
    "        \n",
    "    with tf.name_scope(\"layers\"):\n",
    "        \n",
    "        C1 = ...\n",
    "\n",
    "        C2 = ...\n",
    "\n",
    "        C3 = ...\n",
    "\n",
    "        FC = ...\n",
    "\n",
    "        y_hat = ...\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        cross_entropy = ...\n",
    "        loss = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_pred = tf.equal(tf.argmax(y_hat, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        train_step = ...\n",
    "        \n",
    "    return C1, FC, y_hat, loss, train_step, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vars = get_TF_vars()\n",
    "C1, FC, y_hat, loss, train_step, accuracy = get_TF_graph(tf_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: create optimizer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit (X_train, y_train, X_test, y_test, \n",
    "         model_name, loss, train_step, accuracy, \n",
    "         batch_size, n_epochs, log_freq):\n",
    "    \n",
    "    from rlx import ml\n",
    "\n",
    "    X, Y, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = tf_vars    \n",
    "    \n",
    "    # -----------------\n",
    "    # YOUR CODE HERE\n",
    "    # -----------------\n",
    "        \n",
    "    return log_train, log_test, model_name\n",
    "\n",
    "def plot_results(log_train, log_test):\n",
    "    k = log_train.rolling(window=10).mean().dropna()\n",
    "    plt.plot(k.time, k.accuracy, color=\"blue\", lw=2, label=\"train\")\n",
    "    plt.plot(log_test.time, log_test.accuracy, color=\"red\",lw=2, label=\"test\")\n",
    "    plt.legend(loc=\"center left\", bbox_to_anchor=(1,.5))\n",
    "    plt.plot(log_train.time, log_train.accuracy, alpha=.3, color=\"blue\")\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"elapsed time (secs)\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.axhline(0.5, color=\"black\")\n",
    "    plt.xlim(0,log_train.time.max()+1)\n",
    "    plt.title(\"final train_acc=%.4f, test_acc=%.4f\"%(log_train.accuracy.values[-1], log_test.accuracy.values[-1]))        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vars = get_TF_vars()\n",
    "C1, FC, y_hat, loss, train_step, accuracy = get_TF_graph(tf_vars)\n",
    "\n",
    "log_train, log_test, model_name = fit(train_imgs, train_ohlabs, test_imgs, test_ohlabs,\n",
    "                         \"cnn_cifar10\", loss, train_step, accuracy,\n",
    "                         batch_size=100, n_epochs=10, log_freq=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(log_train, log_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: show confusion matrix (test), misses, filters and sample activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, lr, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = tf_vars    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"models/\"+model_name+\".tf\")    \n",
    "    C1_activations, FC_activations, test_preds, w1,w2,w3,w4,w5 = \\\n",
    "                sess.run([C1,FC, y_hat,W1,W2,W3,W4,W5], feed_dict={X:test_imgs})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show confussion matrix for test imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show first layer filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show some misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show conv1 activations for a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
